from pyspark.sql.functions import (
    col, count, desc, explode, ceil, unix_timestamp, window, sum, when, array_contains, lit, split, to_utc_timestamp,

)
from pyspark.sql import SparkSession
import io
from contextlib import redirect_stdout

"""
Using the below with loop we are gathering all the standard output which mean output genrated by print statements 
and .show() function in this file 
"""
f = io.StringIO()
with redirect_stdout(f):
    spark = SparkSession.builder.appName("Analysis Posts Data").getOrCreate()
    # setting the timezone for spark
    spark.conf.set("spark.sql.session.timeZone", "America/Los_Angeles")
    # below is the path to paraquet file which was generated by prepare_data.py
    posts_path = r'C:\Users\Santosh_Burada\PycharmProjects\hadoop\data\posts_10000000'

    # reading the parquet file
    postsParquet = spark.read.parquet(posts_path)

    print("===========Schema of Posts DataFrame============")
    postsParquet.printSchema()

    # Selecting Specific field's and caching them for further ue
    posts = postsParquet.select(
        'Id',
        'PostTypeId',
        'AcceptedAnswerId',
        'ViewCount',
        'Title',
        'OwnerUserId',
        'CreationDate',
        'Tags',
        'AnswerCount'
    ).cache()

    print("++++++++++++++++++++++++++++++Compute the counts+++++++++++++++++++++++++++++++++++++++++++++++++")

    # applying spark filters
    questions = posts.filter(col('PostTypeId') == 1)
    answers = posts.filter(col('PostTypeId') == 2)

    print("Number of Question in Posts Dataset: ", questions.count())
    print("Number of Answers in Posts Dataset: ", answers.count())

    print("Distinct Number Of Users In Posts Dataset",
          posts.filter(col('OwnerUserId').isNotNull()).select('OwnerUserId').distinct().count())

    AnsweredQuestions = questions.filter(col('AcceptedAnswerId').isNotNull())
    print("Questions which are answered", AnsweredQuestions.count())

    # using Spark Query's
    print("=============list of question with high viewcount to low viewcount=============")
    most_viewed = (
        questions.select('AcceptedAnswerId',
                         'ViewCount',
                         'Title',
                         'AnswerCount',
                         'CreationDate'
                         )
        .orderBy(desc(col('ViewCount').cast("int")))
    )
    most_viewed.show(50, truncate=False)

    print("=============================Compute the response time====================================================")
    response_time = (
        AnsweredQuestions.alias('questions')
        .join(answers.alias('answers'), col('questions.AcceptedAnswerId') == col('answers.Id'))
        .select(
            col('questions.Id'),
            to_utc_timestamp(col('questions.CreationDate'), "America/Montreal").alias('question_time'),
            to_utc_timestamp(col('answers.CreationDate'), "America/Montreal").alias('answer_time')
        )
        .withColumn('response_time',
                    unix_timestamp(to_utc_timestamp('answer_time', "America/Los_Angeles")) - unix_timestamp(
                        to_utc_timestamp('question_time', "America/Los_Angeles")))
        .filter(col('response_time') > 0)
        .orderBy('response_time')
    )

    response_time.show(response_time.count(), False)
    print("==============================hourly_data========================================")
    print('questions were answered in each hour after they were posted.')
    hourly_data = (
        response_time
        .withColumn('hours', ceil(col('response_time') / 3600))
        .groupBy('hours')
        .agg(count('*').alias('cnt'))
        .orderBy('hours')
        .limit(24)
    )
    hourly_data.show(hourly_data.count(), False)

    print("==================The time evolution of the number of questions and answeres====================")
    posts_grouped = (
        posts
        .filter(col('OwnerUserId').isNotNull())
        .groupBy(
            window('CreationDate', '1 week')
        )
        .agg(
            sum(when(col('PostTypeId') == 1, lit(1)).otherwise(lit(0))).alias('questions'),
            sum(when(col('PostTypeId') == 2, lit(1)).otherwise(lit(0))).alias('answers')
        )
        .withColumn('date', col('window.start').cast('date'))
        .orderBy('date')
    )

    posts_grouped.show(posts_grouped.count(), False)

    print("=============Compute number of tags===============")
    #
    tags = (
        questions
        .select('Id',
                'PostTypeId',
                'AcceptedAnswerId',
                'ViewCount',
                'OwnerUserId',
                'CreationDate',
                'Tags',
                'AnswerCount')
        .withColumn('tags', split('tags', '><'))
        .selectExpr(
            '*',
            "TRANSFORM(tags, value -> regexp_replace(value, '(>|<)', '')) AS tags_arr"
        )
    )
    tags.show(n=100, truncate=False)

    #
    print("==========See most popular tags============")

    #
    (
        questions
        .withColumn('tags', split('tags', '><'))
        .selectExpr(
            '*',
            "TRANSFORM(tags, value -> regexp_replace(value, '(>|<)', '')) AS tags_arr"
        )
        .withColumn('tag', explode(col('tags_arr')))
        .groupBy('tag')
        .agg(count('*').alias('tag_frequency'))
        .orderBy(desc('tag_frequency'))
    ).show(n=10,truncate=False)

file = open("output.txt", "w")

# file = open('gs://dataproc-staging-us-central1-291378718946-mvsxebny/notebooks/jupyter/output.txt', 'w')
out = f.getvalue()

file.writelines(out)
